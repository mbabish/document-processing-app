version: '3.8'

services:

  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    environment:
        - OLLAMA_MODEL=mistral:latest
    volumes:
      - ollama_data:/root/.ollama
    expose:
      - "11434:11434"
    restart: unless-stopped

  llm:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm
    expose:
      - "8000:8000"
    environment:
      - OLLAMA_API_BASE=http://ollama:11434/api
      - OLLAMA_MODEL=mistral:latest
    depends_on:
      - ollama
    restart: unless-stopped

  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.backend
    # Don't expose ports directly to host - only through frontend proxy
    expose:
      - "5000"
    volumes:
      - ../backend:/app
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app.py
      - FLASK_DEBUG=1
      - PYTHONUNBUFFERED=1
      - LLM_API_URL=http://llm:8000
    depends_on:
      - llm
    restart: unless-stopped

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.frontend
    ports:
      - "3000:3000"
    volumes:
      - ../frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    environment:
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
      - WDS_SOCKET_PORT=3000
      - FAST_REFRESH=false
    stdin_open: true
    restart: unless-stopped


volumes:
  ollama_data: