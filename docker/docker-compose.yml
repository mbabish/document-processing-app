version: '3.8'

services:
  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.backend
    # Don't expose ports directly to host - only through frontend proxy
    expose:
      - "5000"
    volumes:
      - ../backend:/app
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app.py
      - FLASK_DEBUG=1
      - PYTHONUNBUFFERED=1
      - LLM_API_URL=http://llm:8000
    depends_on:
      - llm
    restart: unless-stopped

  frontend:
    build:
      context: ..
      dockerfile: docker/Dockerfile.frontend
    ports:
      - "3000:3000"
    volumes:
      - ../frontend:/app
      - /app/node_modules
    depends_on:
      - backend
      - llm
    environment:
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
      - WDS_SOCKET_PORT=3000
      - FAST_REFRESH=false
    stdin_open: true
    restart: unless-stopped

  llm:
    build:
      context: ..
      dockerfile: docker/Dockerfile.llm
    # Don't expose ports directly to host - only through frontend proxy
    expose:
      - "8000"
    volumes:
      - ../llm:/app
      - llm-cache:/root/.cache  # Persistent cache for model downloads
    environment:
      - LOG_LEVEL=INFO

    restart: unless-stopped
    # If your machine has limited memory, you might want to limit the container
    deploy:
      resources:
        limits:
          memory: 4G  # Adjust based on your system's available memory

volumes:
  llm-cache:  # Define a named volume for the model cache